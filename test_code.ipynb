{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code\n",
    "\n",
    "This is code that was trying to get to work on the project. It may be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the values such as E909, V07, and ? to integer values\n",
    "# df['diag_1'] = df['diag_1'].str.replace('E', '1', regex=False)\n",
    "# df['diag_2'] = df['diag_2'].str.replace('E', '1', regex=False)\n",
    "# df['diag_3'] = df['diag_3'].str.replace('E', '1', regex=False)\n",
    "\n",
    "# df['diag_1'] = df['diag_1'].str.replace('V', '20', regex=False)\n",
    "# df['diag_2'] = df['diag_2'].str.replace('V', '20', regex=False)\n",
    "# df['diag_3'] = df['diag_3'].str.replace('V', '20', regex=False)\n",
    "\n",
    "# df['diag_1'] = df['diag_1'].str.replace('?', '0', regex=False)\n",
    "# df['diag_2'] = df['diag_2'].str.replace('?', '0', regex=False)\n",
    "# df['diag_3'] = df['diag_3'].str.replace('?', '0', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Preprocessing steps for numeric columns\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Preprocessing steps for categorical columns\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))  # Use drop='first' to avoid dummy variable trap\n",
    "# ])\n",
    "\n",
    "# # Apply column transformations to the appropriate columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "\n",
    "# # Combine the training and test datasets\n",
    "# X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "# y_combined  = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "# # Preprocess the combined dataset\n",
    "# X_combined_processed = preprocessor.fit_transform(X_combined)\n",
    "\n",
    "# # Split the combined dataset back into training and test datasets\n",
    "# X_train_2, X_test_2, y_train, y_test = train_test_split(X_combined_processed, y_combined, test_size=0.25, random_state=13)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# # Fit the preprocessing steps and transform the training data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the encoder from the preprocessor\n",
    "# encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "\n",
    "# # Get the list of feature names after preprocessing\n",
    "# numeric_feature_names = numeric_features\n",
    "# categorical_feature_names = encoder.get_feature_names_out(input_features=categorical_features)\n",
    "\n",
    "# # Print the lengths of numeric and categorical feature names\n",
    "# print(\"Numeric feature names length:\", len(numeric_feature_names))\n",
    "# print(\"Categorical feature names length:\", len(categorical_feature_names))\n",
    "\n",
    "# # Combine numeric and categorical processed data\n",
    "# X_train_processed_df = pd.DataFrame(X_train_processed, columns=numeric_feature_names + categorical_feature_names)\n",
    "\n",
    "# # Print the columns\n",
    "# print(X_train_processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming X_train_processed is a NumPy array with multiple columns\n",
    "# X_train_processed_df = pd.DataFrame(X_train_processed, columns=X.columns)\n",
    "\n",
    "# # Print data types of columns\n",
    "# print(\"Data Types of Columns:\")\n",
    "# print(X_train_processed_df.dtypes)\n",
    "\n",
    "# # Check for missing or NaN values\n",
    "# print(\"\\nMissing Values in X_train_processed:\")\n",
    "# print(X_train_processed_df.isnull().sum())\n",
    "\n",
    "# # Check for NaN values specifically (useful for numerical features)\n",
    "# print(\"\\nNaN Values in X_train_processed:\")\n",
    "# print(X_train_processed_df.isna().sum())\n",
    "\n",
    "# # If you want to check for NaN values in categorical features, use this\n",
    "# print(\"\\nNaN Values in Categorical Features:\")\n",
    "# categorical_features = categorical_transformer.named_steps['encoder'].get_feature_names_out(input_features=categorical_features)\n",
    "# print(X_train_processed_df[categorical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'data' is your DataFrame\n",
    "# has_negatives = (df[categorical_features] < 0).any().any()\n",
    "\n",
    "# if has_negatives:\n",
    "#     print(\"The DataFrame contains negative values.\")\n",
    "# else:\n",
    "#     print(\"The DataFrame does not contain negative values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# # Fit the preprocessing steps and transform the training data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of iterations for feature selection\n",
    "# # Also number of features after one-hot encoding\n",
    "# num_iterations = X_train_processed.shape[1]\n",
    "\n",
    "# # Perform parallel feature selection\n",
    "# selected_features = parallel_feature_selection(X_train_processed, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Random Forest classifier\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Fit RF on the training data\n",
    "# with tqdm(total=X_train_processed.shape[1], desc='RF Progress') as pbar:\n",
    "#     for i in range(1, X_train_processed.shape[1] + 1):\n",
    "#         rf.n_features_to_select = i\n",
    "#         rf.fit(X_train_processed, y_train)\n",
    "#         pbar.update()  # Automatically updates the progress bar for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# # Create the SelectKBest object with the chi2 scoring function\n",
    "# k_best_selector = SelectKBest(score_func=chi2, k='all')  # Set k to 'all' to keep all features\n",
    "\n",
    "# # Fit the selector on the training data and target\n",
    "# k_best_selector.fit(X_train, y_train)\n",
    "\n",
    "# # Get the feature scores and p-values\n",
    "# feature_scores = k_best_selector.scores_\n",
    "# p_values = k_best_selector.pvalues_\n",
    "\n",
    "# # Print or inspect the scores and p-values to select the relevant features\n",
    "# print(\"Feature Scores:\", feature_scores)\n",
    "# print(\"P-Values:\", p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the feature scores and p-values\n",
    "# feature_scores = k_best_selector.scores_\n",
    "# p_values = k_best_selector.pvalues_\n",
    "\n",
    "# # Combine feature names, scores, and p-values into a DataFrame for easier analysis\n",
    "# feature_scores_df = pd.DataFrame({'Feature': X_train.columns, 'Score': feature_scores, 'P-Value': p_values})\n",
    "\n",
    "# # Sort the DataFrame by the scores in descending order\n",
    "# feature_scores_df = feature_scores_df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# # Print the top k features based on their scores\n",
    "# k = 20  # You can set k to any desired number\n",
    "# top_k_features = feature_scores_df.head(k)\n",
    "\n",
    "# print(\"Top\", k, \"features:\")\n",
    "# print(top_k_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_selection_iteration(X_train, y_train, num_features_to_select):\n",
    "#     # Create a Random Forest classifier\n",
    "#     rf = RandomForestClassifier()\n",
    "\n",
    "#     # Fit RF on the training data\n",
    "#     rf.fit(X_train, y_train)\n",
    "\n",
    "#     # Get the feature importances\n",
    "#     feature_importances = rf.feature_importances_\n",
    "\n",
    "#     # Sort the feature importances and select the top 'num_features_to_select' indices\n",
    "#     selected_feature_indices = feature_importances.argsort()[::-1][:num_features_to_select]\n",
    "\n",
    "#     return selected_feature_indices\n",
    "\n",
    "# def parallel_feature_selection(X_train, y_train, num_iterations):\n",
    "#     # Create a list to store the selected feature indices for each iteration\n",
    "#     selected_features = []\n",
    "\n",
    "#     # Parallelize the feature selection loop\n",
    "#     with Parallel(n_jobs=-1, verbose=10) as parallel:\n",
    "#         selected_features = parallel(\n",
    "#             delayed(feature_selection_iteration)(X_train, y_train, num_features_to_select)\n",
    "#             for num_features_to_select in range(1, num_iterations + 1)\n",
    "#         )\n",
    "\n",
    "#     return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the number of features before one-hot encoding\n",
    "# num_original_features = len(numeric_features) + len(categorical_features)\n",
    "\n",
    "# # Print the selected features for each iteration\n",
    "# for i, features in enumerate(selected_features, 1):\n",
    "#     # Calculate the actual selected feature indices\n",
    "#     selected_indices = np.where(features)[0]\n",
    "#     # Only consider features within the original range of features\n",
    "#     selected_indices = selected_indices[selected_indices < num_original_features]\n",
    "#     print(f\"Iteration {i}: Selected Features = {selected_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the feature importances\n",
    "# feature_importances = rf.feature_importances_\n",
    "\n",
    "# # Print or analyze the feature importances\n",
    "# print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
