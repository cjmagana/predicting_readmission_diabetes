{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code\n",
    "\n",
    "This is code that was trying to get to work on the project. It may be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Preprocessing steps for numeric columns\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Preprocessing steps for categorical columns\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))  # Use drop='first' to avoid dummy variable trap\n",
    "# ])\n",
    "\n",
    "# # Apply column transformations to the appropriate columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "\n",
    "# # Combine the training and test datasets\n",
    "# X_combined = pd.concat([X_train, X_test], axis=0)\n",
    "# y_combined  = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "# # Preprocess the combined dataset\n",
    "# X_combined_processed = preprocessor.fit_transform(X_combined)\n",
    "\n",
    "# # Split the combined dataset back into training and test datasets\n",
    "# X_train_2, X_test_2, y_train, y_test = train_test_split(X_combined_processed, y_combined, test_size=0.25, random_state=13)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# # Fit the preprocessing steps and transform the training data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the encoder from the preprocessor\n",
    "# encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "\n",
    "# # Get the list of feature names after preprocessing\n",
    "# numeric_feature_names = numeric_features\n",
    "# categorical_feature_names = encoder.get_feature_names_out(input_features=categorical_features)\n",
    "\n",
    "# # Print the lengths of numeric and categorical feature names\n",
    "# print(\"Numeric feature names length:\", len(numeric_feature_names))\n",
    "# print(\"Categorical feature names length:\", len(categorical_feature_names))\n",
    "\n",
    "# # Combine numeric and categorical processed data\n",
    "# X_train_processed_df = pd.DataFrame(X_train_processed, columns=numeric_feature_names + categorical_feature_names)\n",
    "\n",
    "# # Print the columns\n",
    "# print(X_train_processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming X_train_processed is a NumPy array with multiple columns\n",
    "# X_train_processed_df = pd.DataFrame(X_train_processed, columns=X.columns)\n",
    "\n",
    "# # Print data types of columns\n",
    "# print(\"Data Types of Columns:\")\n",
    "# print(X_train_processed_df.dtypes)\n",
    "\n",
    "# # Check for missing or NaN values\n",
    "# print(\"\\nMissing Values in X_train_processed:\")\n",
    "# print(X_train_processed_df.isnull().sum())\n",
    "\n",
    "# # Check for NaN values specifically (useful for numerical features)\n",
    "# print(\"\\nNaN Values in X_train_processed:\")\n",
    "# print(X_train_processed_df.isna().sum())\n",
    "\n",
    "# # If you want to check for NaN values in categorical features, use this\n",
    "# print(\"\\nNaN Values in Categorical Features:\")\n",
    "# categorical_features = categorical_transformer.named_steps['encoder'].get_feature_names_out(input_features=categorical_features)\n",
    "# print(X_train_processed_df[categorical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'data' is your DataFrame\n",
    "# has_negatives = (df[categorical_features] < 0).any().any()\n",
    "\n",
    "# if has_negatives:\n",
    "#     print(\"The DataFrame contains negative values.\")\n",
    "# else:\n",
    "#     print(\"The DataFrame does not contain negative values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the data into feature and target\n",
    "# X = feature_df\n",
    "# y = target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# # Fit the preprocessing steps and transform the training data\n",
    "# X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# # Transform the testing data using the fitted preprocessor\n",
    "# X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of iterations for feature selection\n",
    "# # Also number of features after one-hot encoding\n",
    "# num_iterations = X_train_processed.shape[1]\n",
    "\n",
    "# # Perform parallel feature selection\n",
    "# selected_features = parallel_feature_selection(X_train_processed, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Random Forest classifier\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# # Fit RF on the training data\n",
    "# with tqdm(total=X_train_processed.shape[1], desc='RF Progress') as pbar:\n",
    "#     for i in range(1, X_train_processed.shape[1] + 1):\n",
    "#         rf.n_features_to_select = i\n",
    "#         rf.fit(X_train_processed, y_train)\n",
    "#         pbar.update()  # Automatically updates the progress bar for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
